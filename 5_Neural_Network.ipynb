{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMq19ySz7BxLOZDDDlw+jSq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oydd_TekW5Xo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Nueral Network\n",
        "\n"
      ],
      "metadata": {
        "id": "OJNB3Fb8XA7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neuron\n",
        "- Neuron has linear part, and activation part → neuron = linear($wx+b$) + activation($\\sigma$)\n",
        "- Model consisted to architecture, and parameter → Model = Architecture + parameter\n",
        "\n",
        "- Notation:\n",
        "    - neuron $a_1^{[1]}$: [1] - layer of neuron , 1 - index in such layer,\n",
        "    - linear part $z_1^{[1]}$: same\n",
        "\n",
        "###How to Learn Neuron\n",
        "(e. g. To determine what animal is in an image)\n",
        "1. initialize $w, b$\n",
        "2. Find the optimal $w, b$ (train)\n",
        "    \n",
        "    → $L = -[y\\log\\hat{y}+(1-y)\\log(1-\\hat{y}) ]$(Loss function, y=0 or 1)\n",
        "    \n",
        "    → $w = w - \\alpha\\frac{\\partial L}{\\partial w}$,  $b = b - \\alpha\\frac{\\partial L}{\\partial b}$\n",
        "    \n",
        "3. use $\\hat{y}=\\sigma(wx+b)$ to predict (activation part)\n",
        "\n",
        "(+) $ \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}}= \\frac {\\hat{y}-y}{\\hat{y}(1-\\hat{y})}$, $\\frac{\\partial \\mathcal{L}}{\\partial z}=\\hat{y}-y$, $\\frac{\\partial \\mathcal{L}}{\\partial w}=(\\hat{y}-y)x$,\n",
        "\n",
        "###Characteristic of Neuron\n",
        "\n",
        "1. is it cat or not → one neuron\n",
        "2. is it cat or lion or iguana → three neuron → it can learn image that contain multiple animals\n",
        "    \n",
        "    - $L$ cant match to three neuron (only represent one classification)\n",
        "    \n",
        "    - $L_{3N} = \\sum_{k=1}^3 [y_k\\log\\hat{y}_k+(1-y_k)\\log(1-\\hat{y_k})]$  \n",
        "    (binary cross-entopy, loss of logistic regression)\n",
        "    \n",
        "3. only one animal in each image? →   softmax - $\\exp(z_1^{[1]}/\\sum_iz_i^{[1]})$\n",
        "    \n",
        "    - output depends on other neuron’s outcomes, not logistic\n",
        "    \n",
        "    - can’t use gradient descent, no logistic\n",
        "     \n",
        "    - cross-entropy Loss: $L_{CE} = -\\sum_{k=1}^3y_k\\log \\hat{y}_k$\n",
        "    \n",
        "4. getting an age of cat\n",
        "    \n",
        "    - change activation function → ReLU\n",
        "    \n",
        "    - change Loss function → $|y-\\hat{y}|$ or squared"
      ],
      "metadata": {
        "id": "FEjCVYkb4wty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Notation\n",
        "- layer: not connected to each other\n",
        "\n",
        "- hidden layer: not directly observable from input, or output\n",
        "\n",
        "**Why hidden layer**: may each neurons detect different characteristic of inputs\n",
        "\n",
        "- (e. g. house price prediction) \\\n",
        "inputs: bedroom, size, zip code, wealth \\\n",
        "→ second edges(hidden layer): family size, walkable, school quality, \\\n",
        "→ output: price\n",
        "\n",
        "- Often, network can determine these features better than human\n",
        "→ blackbox model, end to end learning\n",
        "\n",
        "##Propagation Equations\n",
        "\n",
        "(i.g. 3 first layer, 2 second layer, 1 last layer.)\n",
        "\n",
        "- $z^{[1]} = w^{[1]}x + b$\n",
        "- $z^{[2]} = w^{[2]}a^{[1]}+b^{[2]}$ …\n",
        "    \n",
        "    → $w^{[n]}$ = [# layer [n], # layer n-1 (for n=1, input)],     $b^{[n]}$ = [# layer [n], 1]\n",
        "    \n",
        "    → $z^{[n]}$ = [# layer [n], 1],   $x$ = [# input, 1]\n",
        "    \n",
        "- $a^{[1]} = \\sigma(z^{[1]})$\n",
        "- $a^{[2]}= \\sigma(z^{[2]})$ …\n",
        "    \n",
        "    → $a^{[n]}$ = same to z\n",
        "    \n",
        "\n",
        "For bunch of test case → parrarelize computation\n",
        "\n",
        "- $Z^{[1]} = w^{[1]}x + b^{[1]}$\n",
        "- $Z^{[2]} = w^{[2]}A^{[1]}+b^{[2]}$…\n",
        "    \n",
        "    → $w^{[n]}$ = [# layer [n], # layer n-1 (for n=1, input)],     $b^{[n]}$(broadcasting) = [# layer [n], m]\n",
        "    \n",
        "    → $Z^{[n]}$ = [# layer [n], # examples],   $x$ = [# input, # examples]\n",
        "    \n",
        "- $A^{[1]} = \\sigma(Z^{[1]})$\n",
        "- $A^{[2]}= \\sigma(Z^{[2]})$ …\n",
        "    \n",
        "    → $a^{[n]}$ = same to z\n",
        "    \n",
        "\n",
        "##Backward Propagation\n",
        "\n",
        "Define loss/cost function (1 example for loss, n example for cost)\n",
        "\n",
        "$$\n",
        "J(\\hat{y}, y) = \\frac1m\\sum L^{(i)}  \\quad L = -[y^{(i)}\\log\\hat{y}^{(i)}+(1-y^{(i)})\\log(1-\\hat{y}^{(i)}) ]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\forall l=1\\dots3: \\begin{cases}\n",
        "w^{[l]} = w^{[l]}-\\alpha\\frac{\\partial J}{\\partial w^{[l]}} \\\\\n",
        "b^{[l]} = b^{[l]}-\\alpha\\frac{\\partial J}{\\partial b^{[l]}}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "We should start from the end (closest to y hat)\n",
        "\n",
        "### Derivative of Loss function\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(i)} =a^{[3]}  \\\\\n",
        "\\frac{\\partial L}{\\partial w^{[3]}}=-[y^{(i)}\\frac1{a^{[3]}}\\cdot a^{[3]}(1-a^{[3]})a^{[2]T} + \\\\ (1-y^{(i)})\\frac1{1-a^{[3]}}\\cdot (-1) a^{[3]}(1-a^{[3]})a^{[2]T}] \\\\\n",
        "= -(y^{(i)}-a^{[3]})a^{[2]T} =\n",
        "\\frac{\\partial L}{\\partial z^{[3]}} \\cdot\n",
        "\\frac{\\partial z^{[3]}}{\\partial w^{[3]}}\n",
        "$$\n",
        "\n",
        "(why a2 transpose? derivate w3 should be shape of w3 [1,2], a2 is [2,1])\n",
        "\n",
        "$$\n",
        "\\frac {\\partial J}{\\partial w^{[3]} } = \\frac{-1}m\\sum_{i=1}^m (y^{(i)}-a^{[3]})a^{[2]T}\n",
        "$$\n",
        "\n",
        "→ Backward Propagation\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w^{[2]}}= \\frac{\\partial L}{\\partial a^{[3]}} \\cdot \\frac{\\partial a^{[3]}}{\\partial z^{[3]}} \\cdot\n",
        "\\frac{\\partial z^{[3]}}{\\partial a^{[2]}} \\cdot\n",
        "\\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\cdot\n",
        "\\frac{\\partial z^{[2]}}{\\partial w^{[2]}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (a^{[3]}-y)\\cdot w^{[3]T}\\cdot a^{[2]}(1-a^{[2]})\\cdot a^{[1]T}\n",
        "$$\n",
        "\n",
        "→ Check it\n",
        "\n",
        "- Result: [2, 3]\n",
        "- First term: [1, 1]\n",
        "- Second term: [2, 1]\n",
        "- Third term: [2, 1]\n",
        "- Fourth term: [1, 3]   → How to compute?\n",
        "- Derivative of sigmoid(Third term) is element wise product…\n",
        "\n",
        "→ ( Second * Third [element wise] )( First )( Fourth ) → [2, 3]\n",
        "\n",
        "Save all values (a1 or a2 …) to avoid recomputing terms."
      ],
      "metadata": {
        "id": "_zO7DG9a9IZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "np.random.seed(0)\n",
        "N_samples = 300\n",
        "n_features = 2\n",
        "n_classes = 3\n",
        "X, Y = make_blobs(n_samples=N_samples, n_features=n_features, centers=n_classes,\n",
        "                  cluster_std=1.5, random_state=42)\n",
        "\n",
        "Y = Y.reshape(-1, 1)\n",
        "NewY = np.zeros((Y.shape[0], n_classes))\n",
        "for i in range(Y.shape[0]):\n",
        "    NewY[i][Y[i][0]] = 1\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X[:, 0], X[:, 1], c = NewY, label='Data', alpha=0.8)\n",
        "plt.title('Synthetic Data for Softmax Regression (3 Classes)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Class Label')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, NewY, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xv106FVk9qEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris # scikit-learn의 샘플 데이터 로드를 위해 import\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "Y = iris.target\n",
        "Y = Y.reshape(-1, 1)\n",
        "NewY = np.zeros((Y.shape[0], n_classes))\n",
        "for i in range(Y.shape[0]):\n",
        "    NewY[i][Y[i][0]] = 1\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X[:, 0], X[:, 1], c = Y, label='Data', alpha=0.8)\n",
        "plt.title('Synthetic Data for Softmax Regression (3 Classes)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Class Label')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, NewY, test_size=0.2, random_state=42)\\\n",
        "\n",
        "X_train = X\n",
        "Y_train = NewY\n",
        "X_test = X\n",
        "Y_test = Y"
      ],
      "metadata": {
        "id": "KyZbGeC01q8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "linear = lambda x: x\n",
        "ReLU = lambda x: np.maximum(0,x)\n",
        "\n",
        "def one_hot_encode(Y, num_classes):\n",
        "    N = Y.shape[0]\n",
        "    one_hot_Y = np.zeros((N, num_classes))\n",
        "    # Ensure Y is flattened to 1D for advanced indexing\n",
        "    one_hot_Y[np.arange(N), Y.flatten()] = 1\n",
        "    return one_hot_Y\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, architecture: list):\n",
        "        self.architecture = architecture\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "        length = len(architecture)\n",
        "        for i in range(length-2):\n",
        "            self.layers.append(Dense(architecture[i], architecture[i+1], ReLU, i))\n",
        "\n",
        "        self.layers.append(Dense(architecture[-2], architecture[-1], linear, length-1))\n",
        "        self.layers.append(OutSoftmax())\n",
        "\n",
        "    #x: np.array[feature, 1]\n",
        "    def propagation(self, x):\n",
        "        prev = x\n",
        "        for layer in self.layers:\n",
        "            prev = layer.forward(prev)\n",
        "        return prev\n",
        "\n",
        "    def back_propagation(self, x, y_hat, y_res):\n",
        "        dloss = y_hat\n",
        "        for layer in reversed(self.layers):\n",
        "            dloss = layer.backward(dloss, self.alpha)\n",
        "\n",
        "    def learn_network(self, x, y_true, epoch = 100, alpha=0.01):\n",
        "        self.alpha = alpha\n",
        "        num_classes = self.architecture[-1]\n",
        "\n",
        "        for cnt in range(epoch):\n",
        "            y_res = self.propagation(x)\n",
        "            self.back_propagation(x, y_true, y_res)\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.params = {}\n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, dout):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size, activation, layerindex):\n",
        "        super().__init__()\n",
        "        std_dev = np.sqrt(2 / input_size)\n",
        "        self.params['w'] = np.random.randn(input_size, output_size) * std_dev\n",
        "        self.params['b'] = np.zeros((1, output_size))\n",
        "        self.activation = activation\n",
        "        self.layerindex = layerindex\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = x @ self.params['w']+ np.tile(self.params['b'],(x.shape[0],1))\n",
        "        out = self.activation(out)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout, alpha):\n",
        "        w = self.params['w']\n",
        "        if self.activation == sigmoid:\n",
        "            dZ = dout * (self.out * (1 - self.out))\n",
        "        elif self.activation == ReLU:\n",
        "            dZ = dout * (self.out > 0)\n",
        "        else: # linear\n",
        "            dZ = dout\n",
        "\n",
        "        dw = self.x.T @ dZ\n",
        "        db = np.sum(dZ, axis=0)\n",
        "        dx = dZ @ w.T\n",
        "\n",
        "        self.params['w'] -= alpha * dw\n",
        "        self.params['b'] -= alpha * db\n",
        "        #print(f\"Layer ({self.layerindex}) W: {self.params['w'] }, b: {self.params['b']}\", end=\"\")\n",
        "        return dx\n",
        "\n",
        "class OutSoftmax(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (lambda e: np.exp(e))(x)\n",
        "        sums = np.sum(out, axis = 1)\n",
        "        sums = sums.reshape(-1,1)\n",
        "        sums = sums.repeat(out.shape[1], axis=1)\n",
        "        out = out / sums\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, y_hat, alpha):\n",
        "        #print()\n",
        "        return (self.out - y_hat) / y_hat.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "net = Network([4,10,10,3])\n",
        "net.learn_network(X_train, Y_train, 100, 0.1)"
      ],
      "metadata": {
        "id": "9E1k3GGRksaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_result = net.propagation(X_test)\n",
        "\n",
        "def get_predicted_classes(Y_probs):\n",
        "    return np.argmax(Y_probs, axis=1)\n",
        "\n",
        "print(\"Raw probabilities from network:\\n\", Y_result)\n",
        "\n",
        "predicted_classes = get_predicted_classes(Y_result)\n",
        "\n",
        "print(\"Predicted class labels:\\n\", predicted_classes)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "# Use the predicted_classes (integer labels) for coloring\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=predicted_classes, label='Data', alpha=0.8)\n",
        "plt.title('Synthetic Data for Softmax Regression (3 Classes)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Class Label')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a0vkjqXi3xs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}