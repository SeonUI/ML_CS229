{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORCif+pac74o4hjYyxX6YM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regressions"
      ],
      "metadata": {
        "id": "djhWt1HPsMW3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03503b8c"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9d5b7ad9"
      },
      "source": [
        "# Generate sample data\n",
        "X = 100 * np.random.rand(100, 1) # Features\n",
        "Y = 40 + 3 * X + 40 * np.random.randn(100, 1) # Target (linear relationship + noise)\n",
        "\n",
        "# Visualize the generated data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, Y, color='blue', label='Original Data')\n",
        "plt.title('Synthetic Data for Linear Regression')\n",
        "plt.xlabel('X (Feature)')\n",
        "plt.ylabel('Y (Target)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Generated X (first 5 rows):\\n\", X[:5])\n",
        "print(\"Generated Y (first 5 rows):\\n\", Y[:5])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression\n",
        "- $\\theta$: parameters\n",
        "- $\\theta_j$ = jth parameters → $ℝ^{n+1}$ when n = # input\n",
        "- m = # trainning examples\n",
        "- x = inputs / features\n",
        "- y = output / target\n",
        "- $(x, y)$ = training examples\n",
        "- $(x^{(i)}, y^{(i)} )$ = ith training examples\n",
        "\n",
        "**Hypothesis Function**:  $h(x) = \\theta_0+\\theta_1x_1+\\theta_2x_2+...$\n",
        "\n",
        "**Cost Function**: $J(\\theta) =\\frac12\\sum_{i=1}^m(h_\\theta(x^i)-y^i)^2$\n",
        "\n",
        "GOAL : choose Parameters s.t. $h(x) \\approx y$ for target datas.\n",
        "\n",
        "→ minimize Cost Function → gradient descent"
      ],
      "metadata": {
        "id": "vOyvcz5JrQRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Gradient Descent\n",
        "\n",
        "(calcutating can be slow with huge dataset)\n",
        "\n",
        "- start with random $\\theta$ → keep changing $\\theta$ to reduce $J(\\theta)$\n",
        "\n",
        "- Find partial derivative of $J(\\theta)$:   $\\frac{\\delta}{\\delta \\theta_j}J(\\theta) = (h_\\theta(x)-y)⋅ x_j$\n",
        "\n",
        "- $\\theta_j ≔  \\theta_j - \\alpha (h_\\theta(x)-y)⋅x_j$    ($\\alpha$ as learning weight)\n",
        "\n",
        "Repeat until convergence → find local optimal"
      ],
      "metadata": {
        "id": "RoCTUxvAzDiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_batch(X_train, Y_train, alpha = 0.0001, epoch = 100):\n",
        "  intercept = np.random.rand() #theta 0\n",
        "  coefficient = np.random.rand(1,1) #theta 1\n",
        "\n",
        "  # Fit Model\n",
        "  for i in range(epoch):\n",
        "    error = intercept + coefficient * X_train - Y_train\n",
        "    intercept = intercept - alpha * np.mean(error)\n",
        "    coefficient = coefficient - alpha * np.mean(error) * X_train\n",
        "    print(f\"Epoch {i+1}: Intercept = {intercept:.2f}, Coefficient = {coefficient[0][0]:.2f}\")\n",
        "\n",
        "  print(f\"Intercept (theta_0): {intercept:.2f}\")\n",
        "  print(f\"Coefficient (theta_1): {coefficient[0][0]:.2f}\")\n",
        "  return intercept, coefficient\n",
        "\n",
        "intercept, coefficient =  linear_regression_batch(X_train, Y_train)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "npT4aKOsyxzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochastic gradient descent\n",
        "update after use one data → memory efficient\n",
        "\n",
        "Descending makes some noises but eventually goes to local minimum"
      ],
      "metadata": {
        "id": "I2QW75wf-mTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_stochastic(X_train, Y_train, alpha = 0.001, epoch = 100):\n",
        "  alpha = 0.0001\n",
        "  epoch = 100\n",
        "  intercept = np.random.rand() #theta 0\n",
        "  coefficient = np.random.rand() #theta 1\n",
        "\n",
        "  # Fit Model\n",
        "  for i in range(epoch):\n",
        "    for j in range(len(X_train)):\n",
        "      error = intercept + coefficient * X_train[j].item() - Y_train[j].item()\n",
        "      intercept = intercept - alpha * error\n",
        "      coefficient = coefficient - alpha * error * X_train[j].item()\n",
        "    print(f\"Epoch {i+1}: Intercept = {intercept:.2f}, Coefficient = {coefficient:.2f}\")\n",
        "\n",
        "  print(f\"Intercept (theta_0): {intercept:.2f}\")\n",
        "  print(f\"Coefficient (theta_1): {coefficient:.2f}\")\n",
        "  return intercept, coefficient\n",
        "\n",
        "intercept, coefficient = linear_regression_stochastic(X_train, Y_train)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PbOrHfQz-lgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normal Equation\n",
        "$\\theta = (x^Tx)^{-1}x^Ty$\n",
        "\n",
        "Derivation:\n",
        "1. $x\\theta = \\text{(column vector)}x^{(i)^T}\\theta =\\text{(column vector)} h(x)$\n",
        "\n",
        "2. $J(\\theta) =\\frac12\\sum_{i=1}^m(h_\\theta(x^i)-y^i)^2 = \\frac12\\sum_{i=1}^m(x\\theta -y)^2$ (Matrix)\n",
        "\n",
        "3. $\\frac12\\sum_{i=1}^m(x\\theta -y)^2 = \\frac12(x\\theta - y)^T(x\\theta - y)$\n",
        "\n",
        "4. $\\nabla_\\theta \\frac12(x\\theta - y)^T(x\\theta - y) = (x^Tx)^{-1}x^Ty$\n",
        "\n",
        "\n",
        "### +) Linear algebra\n",
        "\n",
        "- Derivative of Matrix function → partial derivative of each element of matrix\n",
        "\n",
        "- trace of A → sum of diagonal entries, tr(A)\n",
        "\n",
        "- $trA = trA^T$\n",
        "\n",
        "- $trAB = trBA$,         $trABC = trCAB$\n",
        "\n",
        "- $f(A) = trAB$  → $\\nabla _A f(A) = B^T$\n",
        "\n",
        "- $\\nabla_A trAA^TC = CA+C^TA$\n",
        "\n",
        "- $\\sum_{i=1,\\ j=i}^{i=n,\\ j=m}(a_{ij})^2 = A^TA$"
      ],
      "metadata": {
        "id": "llM1iEdLNdjY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f54646a3"
      },
      "source": [
        "def linear_regression_normal_equation(X_train, Y_train):\n",
        "  # Add a bias (intercept) term to X_train\n",
        "  X_b_train = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
        "\n",
        "  # Normal Equation\n",
        "  theta_normal = np.linalg.inv(X_b_train.T @ X_b_train) @ X_b_train.T @ Y_train\n",
        "\n",
        "  # Extract intercept and coefficient\n",
        "  intercept_normal = theta_normal[0][0]\n",
        "  coefficient_normal = theta_normal[1][0]\n",
        "\n",
        "  print(f\"Intercept (theta_0) from Normal Equation: {intercept_normal:.2f}\")\n",
        "  print(f\"Coefficient (theta_1) from Normal Equation: {coefficient_normal:.2f}\")\n",
        "  return intercept_normal, coefficient_normal\n",
        "\n",
        "intercept, coefficient = linear_regression_normal_equation(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Estimation\n",
        "\n",
        "def gradient_model_estimation(intercept, coefficient)\n",
        "  #Make Predictions\n",
        "  Y_pred = intercept + coefficient * X_test\n",
        "\n",
        "  #Evaluate model\n",
        "  mse = mean_squared_error(Y_test, Y_pred)\n",
        "  r2 = r2_score(Y_test, Y_pred)\n",
        "  print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "  print(f\"R-squared (R2): {r2:.2f}\")\n",
        "\n",
        "#gradient_model_estimation(intercept, coefficient)"
      ],
      "metadata": {
        "id": "yzNQahQL6rdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+) LinearRegression model from scikit"
      ],
      "metadata": {
        "id": "137wYB9gSUut"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f6388a8"
      },
      "source": [
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "model.fit(X_train, Y_train)     # It actually uses Singular Value Decomposition (SVD)\n",
        "\n",
        "# The intercept (theta_0)\n",
        "print(f\"Intercept (theta_0): {model.intercept_[0]:.2f}\")\n",
        "# The coefficient (theta_1)\n",
        "print(f\"Coefficient (theta_1): {model.coef_[0][0]:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bb6eb38"
      },
      "source": [
        "Y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(Y_test, Y_pred)\n",
        "r2 = r2_score(Y_test, Y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R-squared (R2): {r2:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "75e9eb0f"
      },
      "source": [
        "# Plot the regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_test, Y_test, color='blue', label='Actual Test Data')\n",
        "plt.plot(X_test, Y_pred, color='red', linewidth=2, label='Regression Line')\n",
        "plt.title('Linear Regression Fit')\n",
        "plt.xlabel('X (Feature)')\n",
        "plt.ylabel('Y (Target)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Locally Weighted Regression\n",
        "\n",
        "- Non-parametric learning algorithm\n",
        "- $J(\\theta) = \\sum_{i=1}^{m}w^{(i)}(y^{(i)}-\\theta^T x^{(i)})^2$\n",
        "- where $w^i$ is weighting fuction, which returns about 1 if $x^i$ is close to target  $x$ or about 0\n",
        "- $w^i = \\exp(-\\frac{{(x^{(i)}-x)}^2}{2\\tau^2})$ ($\\tau$ = bandwidth)\n",
        "-  $|x^{(i)} - x|$is small, i.e. can cover a lot of Xs within bandwidth (goes to 1)\n",
        "-   $|x^{(i)} - x|$is large i.e. can't cover a lot of Xs within bandwidth (goes to 0)  \n",
        "\n",
        "### Non-parametric learning algorithm\n",
        "\n",
        "- parametric learning algorithm: Fit fixed set of parameters to data\n",
        "- Non-parametric learning algorithm: Amount of data/parameters **grows** (linearly) with size of data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rwd7SRbWRjUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample data\n",
        "function = lambda x: np.sin(x)\n",
        "X = 10 * np.random.rand(100, 1) # Features\n",
        "Y =  10 * function(X) + 5 * np.random.randn(100, 1) # Target (linear relationship + noise)\n",
        "\n",
        "# Visualize the generated data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, Y, color='blue', label='Original Data')\n",
        "plt.title('Synthetic Data for Linear Regression')\n",
        "plt.xlabel('X (Feature)')\n",
        "plt.ylabel('Y (Target)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Generated X (first 5 rows):\\n\", X[:5])\n",
        "print(\"Generated Y (first 5 rows):\\n\", Y[:5])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aIf_8NA0XZPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_kernel(x_i, x, tau):\n",
        "    return np.exp(-np.sum((x_i - x)**2) / (2 * tau**2))\n",
        "\n",
        "def locally_weighted_regression(X_train, Y_train, query_point, tau):\n",
        "    m = X_train.shape[0]\n",
        "    weights = np.array([gaussian_kernel(X_train[i], query_point, tau) for i in range(m)])\n",
        "\n",
        "    # Add bias term to X_train for the normal equation\n",
        "    X_b = np.c_[np.ones((m, 1)), X_train]\n",
        "\n",
        "    # Create a diagonal weight matrix\n",
        "    W = np.diag(weights)\n",
        "\n",
        "    # Calculate theta using the weighted normal equation\n",
        "    theta = np.linalg.inv(X_b.T @ W @ X_b) @ X_b.T @ W @ Y_train\n",
        "\n",
        "    # Predict for the query point\n",
        "    prediction = np.array([1, query_point.item()]) @ theta\n",
        "    return prediction[0]\n",
        "\n",
        "def predict_lwr(X_train, Y_train, X_test, tau):\n",
        "    Y_pred = []\n",
        "    for x_test_point in X_test:\n",
        "        Y_pred.append(locally_weighted_regression(X_train, Y_train, x_test_point, tau))\n",
        "    return np.array(Y_pred).reshape(-1, 1)\n",
        "\n",
        "# Set bandwidth parameter\n",
        "tau = 0.5\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred_lwr = predict_lwr(X_train, Y_train, X_test, tau)\n",
        "\n",
        "# Sort the test data and predictions for smoother plotting\n",
        "sorted_indices = np.argsort(X_test.flatten())\n",
        "X_test_sorted = X_test[sorted_indices]\n",
        "Y_pred_lwr_sorted = Y_pred_lwr[sorted_indices]\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(X, Y, color='blue', label='Original Data', alpha=0.6)\n",
        "plt.plot(X_test_sorted, Y_pred_lwr_sorted, color='red', linewidth=3, label=f'LWR Prediction (tau={tau})')\n",
        "plt.title('Locally Weighted Regression')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_0g1-hdFcJnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "1. In classification, we need $h(x)$  fuction with domain $[0,1]$ .\n",
        "2. Sigmoid funtion / logistic fuction $g(x) = \\frac{1}{1+e^{-x}}$ ,   $h(x) = g(\\theta^Tx)$\n",
        "3. Probability function of classification\n",
        "    \n",
        "    $$\n",
        "    P(y=1|x;\\theta) = h_\\theta(x)\\ \\ P(y=0|x;\\theta) = 1 - h_\\theta(x)\n",
        "    $$\n",
        "    \n",
        "    → can be compressed to $P(y|x;\\theta) = h_\\theta(x)^y(1-h_\\theta(x))^{1-y}$ (Bernoulli Distribution)\n",
        "    \n",
        "4. Likelyhood   $L(\\theta) = \\prod_{i=1}^{m} h_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{1-y^{(i)}}$\n",
        "5. choose $\\theta$  to **maximize** log likelyhood $l(\\theta)$\n",
        "    \n",
        "    → Batch Gradient **Ascent**\n",
        "    \n",
        "    $$\n",
        "     \\theta_j = \\theta_j + \\alpha\\frac{\\delta}{\\delta\\theta_j}l(\\theta) = \\theta_j + \\alpha\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)}\n",
        "    $$"
      ],
      "metadata": {
        "id": "T5_FgXCAmPiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilistic Interpretation\n",
        "\n",
        "Assumption: $\\theta^Tx^{(i)}+\\varepsilon^{(i)}$, $\\varepsilon^{(i)}$ is  **error fuction**, which follows Gaussian Distribution\n",
        "\n",
        "This implies $P(y^{(i)}|x^{(i)};\\theta) = N(y^{i}|\\theta^Tx^{(i)}, \\sigma^2)$\n",
        "\n",
        "**Probability of y** when **x**, parametrized as **theta**\n",
        "is equal to **Gaussian distribution** with mean, distribution\n",
        "\n",
        "→ given **x and theta**, the **correct output y** can be distributed as **Gaussian Distribution**\n",
        "\n",
        "Likelyhood function, probability of data:\n",
        "$L(\\theta) = P(\\overrightarrow{y}|\\ x;\\theta) = \\prod_{i=1}^{m}N(\\theta^Tx^{(i)}, \\sigma^2)$\n",
        "\n",
        "use log likelyhood function…"
      ],
      "metadata": {
        "id": "bzjMFZus1inb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNY4zKkvqfpC"
      },
      "source": [
        "N = 300\n",
        "X = 10 * np.random.rand(N, 2)\n",
        "Y = (X @ np.array([[-3], [4]])) + 20 * np.random.rand(N, 1) > 20\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "colors = np.where(Y.flatten() == True, 'b', 'r')\n",
        "plt.scatter(X[:,0], X[:,1], c=colors, label='Data', alpha=0.6)\n",
        "plt.title('Logistic instance')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def logistic_regression(X_train, Y_train, alpha = 0.001, epoch = 100):\n",
        "    theta = np.random.randn(np.shape(X_train)[1], 1)\n",
        "\n",
        "    for i in range(epoch):\n",
        "        theta = theta + alpha * X_train.T @ (Y_train - sigmoid(X_train @ theta))\n",
        "        #print(f\"Epoch {i+1}: theta = {theta}\")\n",
        "\n",
        "    return theta\n",
        "\n",
        "parameter = logistic_regression(X_train, Y_train)\n",
        "\n",
        "plot_x1 = np.array([min(X[:,0]), max(X[:,0])])\n",
        "plot_x2 = plot_x1 * (-parameter[0][0] / parameter[1][0])\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "colors = np.where(Y_test.flatten() == True, 'b', 'r')\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c = colors, label = 'Data', alpha = 0.6)\n",
        "plt.plot(plot_x1, plot_x2, color='orange', linewidth=3, label='prediction')\n",
        "plt.title('Logistic Regression')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sQrHPZkHmPTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}