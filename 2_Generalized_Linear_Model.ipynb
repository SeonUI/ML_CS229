{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOixnhOT99HxlZgT8AXOOb2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBs-QqK8y7u8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generalized Linear Model\n",
        "## 1. Logistic Regression\n",
        "## 2. Perceptron\n",
        "### +) Exponential Family, GLM\n",
        "## 3. Softmax Regression"
      ],
      "metadata": {
        "id": "ERfFjsJtzC1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "1. In classification, we need $h(x)$  fuction with domain $[0,1]$ .\n",
        "2. Sigmoid funtion / logistic fuction $g(x) = \\frac{1}{1+e^{-x}}$ ,   $h(x) = g(\\theta^Tx)$\n",
        "3. Probability function of classification\n",
        "    \n",
        "    $$\n",
        "    P(y=1|x;\\theta) = h_\\theta(x)\\ \\ P(y=0|x;\\theta) = 1 - h_\\theta(x)\n",
        "    $$\n",
        "    \n",
        "    → can be compressed to $P(y|x;\\theta) = h_\\theta(x)^y(1-h_\\theta(x))^{1-y}$ (Bernoulli Distribution)\n",
        "    \n",
        "4. Likelyhood   $L(\\theta) = \\prod_{i=1}^{m} h_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{1-y^{(i)}}$\n",
        "5. choose $\\theta$  to **maximize** log likelyhood $l(\\theta)$\n",
        "    \n",
        "    → Batch Gradient **Ascent**\n",
        "    \n",
        "    $$\n",
        "     \\theta_j = \\theta_j + \\alpha\\frac{\\delta}{\\delta\\theta_j}l(\\theta) = \\theta_j + \\alpha\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)}\n",
        "    $$\n",
        "\n",
        "\n",
        "### +)Probabilistic Interpretation\n",
        "\n",
        "- Assumption: $\\theta^Tx^{(i)}+\\varepsilon^{(i)}$, $\\varepsilon^{(i)}$ is  **error fuction**, which follows Gaussian Distribution\n",
        "\n",
        "- This implies $P(y^{(i)}|x^{(i)};\\theta) = N(y^{i}|\\theta^Tx^{(i)}, \\sigma^2)$\n",
        "\n",
        "- **Probability of y** when **x**, parametrized as **theta**\n",
        "is equal to **Gaussian distribution** with mean, distribution → given **x and theta**, the **correct output y** can be distributed as **Gaussian Distribution**\n",
        "\n",
        "Likelyhood function, probability of data:\n",
        "$L(\\theta) = P(\\overrightarrow{y}|\\ x;\\theta) = \\prod_{i=1}^{m}N(\\theta^Tx^{(i)}, \\sigma^2)$"
      ],
      "metadata": {
        "id": "VmG9Yu0DzH0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "N = 300\n",
        "X = 10 * np.random.rand(N, 2)\n",
        "Y = (X @ np.array([[-3], [4]])) + 20 * np.random.rand(N, 1) > 20\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "colors = np.where(Y.flatten() == True, 'b', 'r')\n",
        "plt.scatter(X[:,0], X[:,1], c=colors, label='Data', alpha=0.6)\n",
        "plt.title('Logistic instance')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "JZ-H5e8UzJsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def logistic_regression(X_train, Y_train, alpha = 0.001, epoch = 100):\n",
        "    theta = np.random.randn(np.shape(X_train)[1], 1)\n",
        "\n",
        "    for i in range(epoch):\n",
        "        theta = theta + alpha * X_train.T @ (Y_train - sigmoid(X_train @ theta))\n",
        "        #print(f\"Epoch {i+1}: theta = {theta}\")\n",
        "\n",
        "    return theta\n",
        "\n",
        "parameter = logistic_regression(X_train, Y_train)\n",
        "\n",
        "plot_x1 = np.array([min(X[:,0]), max(X[:,0])])\n",
        "plot_x2 = plot_x1 * (-parameter[0][0] / parameter[1][0])\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "colors = np.where(Y_test.flatten() == True, 'b', 'r')\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c = colors, label = 'Data', alpha = 0.6)\n",
        "plt.plot(plot_x1, plot_x2, color='orange', linewidth=3, label='prediction')\n",
        "plt.title('Logistic Regression')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yutZDt3ozOLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron\n",
        "1. perceptron function:    $g(x) = \\begin{cases}1 \\quad (x\\ge 0) \\\\ 0 \\quad (x\\lt 0)\\end{cases} $\n",
        "    \n",
        "2. $h(x) = g(\\theta^Tx)$\n",
        "    \n",
        "3. Update Rule: $\\theta_j = \\theta_j + \\alpha(y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)}$\n",
        "    \n",
        "    - $y^{(i)}-h_\\theta(x^{(i)})$ is.. \\\n",
        "        0 → algorithm got it right.      \n",
        "        1 → wrong when y = 1       \n",
        "        -1 →  wrong when y = 0\n",
        "    \n",
        "    - $\\alpha x_j^{(i)}$ to vector addition → tilt classification line towards $x^{(i)}$"
      ],
      "metadata": {
        "id": "3C9epZ5KzRjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron(x):\n",
        "    return (x >= 0).astype(int)\n",
        "\n",
        "def perceptron_regression(X_train, Y_train, alpha = 0.001, epoch = 100):\n",
        "    theta = np.random.randn(np.shape(X_train)[1], 1)\n",
        "    theta\n",
        "\n",
        "    for i in range(epoch):\n",
        "        theta = theta + alpha * X_train.T @ (Y_train - perceptron(X_train @ theta))\n",
        "        #print(f\"Epoch {i+1}: theta = {theta}\")\n",
        "\n",
        "    return theta\n",
        "\n",
        "parameter = perceptron_regression(X_train, Y_train)\n",
        "\n",
        "plot_x1 = np.array([min(X[:,0]), max(X[:,0])])\n",
        "plot_x2 = plot_x1 * (-parameter[0][0] / parameter[1][0])\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "colors = np.where(Y_test.flatten() == True, 'b', 'r')\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c = colors, label = 'Data', alpha = 0.6)\n",
        "plt.plot(plot_x1, plot_x2, color='orange', linewidth=3, label='prediction')\n",
        "plt.title('Logistic Regression')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IeMo-1Y0zPyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential Family\n",
        "\n",
        "distribution with PDF(probability density Function) form with below.\n",
        "$$\n",
        "p(y;\\eta) = b(y)exp[\\eta^TT(y)-a(\\eta)]\n",
        "$$\n",
        "- y - data\n",
        "- $\\eta$ - natural parameter → match dimension with T(y)\n",
        "- $T(y)$ - Sufficicant statistic → eventually goes to y\n",
        "- $b(y)$ - Base measure\n",
        "- $a(\\eta)$ - log-partition\n",
        "\n",
        "HOW to choose?\n",
        "- Real value → Gaussian\n",
        "- Binary → Bernoulli\n",
        "- Count →  Poisson\n",
        "- R^2 →  Gamma/exponential\n",
        "- Distribution →  Bayesian\n",
        "\n",
        "Properties\n",
        "- MLE(Maximum Likelyhood Estimation) wrt $\\eta$ → concave\n",
        "- Mean is partial derivative of log-partition\n",
        "- Variance is second partial deraivative of log-partition\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B842Oe9ZzWXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GLM (Generalized Linear model)\n",
        "\n",
        "Can derive Exp.Fam. with model\n",
        "\n",
        "- Assume $y$ (i.e. $\\mu$) follows Exponential Family.\n",
        "i.e. $P(y|x;\\theta)$ ~ Exp.Fam.($\\eta$)\n",
        "\n",
        "- Linear Components: linear combination of predictor variables and coefficients $ \\mu  = \\theta^Tx$\n",
        "\n",
        "- Link function (g): function that relates $\\mu$ and $\\eta$ ($\\eta = \\theta^Tx$ for Gaussian)\n",
        "\n",
        "\n",
        "Three Parameters\n",
        "\n",
        "- Model param $\\theta$ → learning\n",
        "- Natural Param $\\eta$ → design choose from $\\theta^Tx$\n",
        "- Canonical Param: Each Exp.Fam.'s parameter that derived by $\\eta$  \n",
        "( e.g. Gaussian : $\\mu\\sigma^2$, Bernoulli : $\\phi$, Poisson : $\\lambda$ )"
      ],
      "metadata": {
        "id": "QyJJfPhWzXUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Softmax Regression (Multiclass Classification)\n",
        "make linear classification for each classes\n",
        "\n",
        "Terminology\n",
        "- $K$ - # classes\n",
        "\n",
        "- $y = [\\{0,1\\}^K]$ (e.g. {0,0,1,0} meaning third label.)\n",
        "\n",
        "- $\\theta_c \\in ℝ^n$\n",
        "\n",
        "How to calculate Probability?\n",
        "\n",
        "1. given x\n",
        "\n",
        "2. calc $\\theta_c^Tx$ for each class\\\n",
        " $z = [\\theta_1^Tx,  \\theta_2^Tx, \\theta_3^Tx …]$\n",
        "\n",
        "3. $softmax(z) = [\\frac{e^z_1}{\\sum e^z}, \\frac{e^z_2}{\\sum e^z} , \\frac{e^z_3}{\\sum e^z} … ]$\n",
        "\n",
        "4. One-hot encoding: use argmax of z\n",
        "\n",
        "How to learn?\n",
        "\n",
        "Cost Function: Cross Entropy $J(\\theta) = -\\sum_yy \\log(\\hat y)$  ->\n",
        "\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i}\\sum_{k} y_{k}^{(i)} \\log(\\hat{y}_{k}^{(i)})\n",
        "$$\n",
        "Partial differentiate wrt theta ->\n",
        "$$\n",
        "\\nabla_{\\theta_k} J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} (\\hat y_{k}^{(i)} - {y}_{k}^{(i)})x_i\n",
        "$$\n"
      ],
      "metadata": {
        "id": "FQ1gXkjizach"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "np.random.seed(0)\n",
        "N_samples = 300\n",
        "n_features = 2\n",
        "n_classes = 3\n",
        "X, Y = make_blobs(n_samples=N_samples, n_features=n_features, centers=n_classes,\n",
        "                  cluster_std=1.5, random_state=42)\n",
        "\n",
        "Y = Y.reshape(-1, 1)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=Y.flatten(), label='Data', alpha=0.8)\n",
        "plt.title('Synthetic Data for Softmax Regression (3 Classes)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Class Label')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_15U_Bnbzcz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(theta, X):\n",
        "    # subtract constant to avoid overflow, same result\n",
        "    comb = X @ theta\n",
        "    exp = np.exp(comb - np.max(comb, axis=1, keepdims=True))\n",
        "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "def softmax_regression(X_train, Y_train, n_class, alpha = 0.001, epoch = 100):\n",
        "    m = np.shape(X_train)[0]\n",
        "    # Add bias term\n",
        "    X_train = np.c_[np.ones((m, 1)), X_train]\n",
        "    features = np.shape(X_train)[1]\n",
        "\n",
        "    # Initialize theta [features, n_class]\n",
        "    theta = np.random.randn(features, n_class) * 0.1\n",
        "\n",
        "    # Convert Y_train to one-hot encoding\n",
        "    Y_train_one_hot = np.zeros((m, n_class))\n",
        "    Y_train_one_hot[np.arange(m), Y_train.flatten()] = 1\n",
        "\n",
        "    for i in range(epoch):\n",
        "        Y_pred = softmax(theta, X_train)\n",
        "        # Gradient calculation\n",
        "        gradient = (1 / m) * X_train.T @ (Y_pred - Y_train_one_hot)\n",
        "        theta = theta - alpha * gradient\n",
        "\n",
        "    return theta\n",
        "\n",
        "parameter = softmax_regression(X_train, Y_train, n_classes)\n",
        "print(\"Learned Parameters (theta):\")\n",
        "print(parameter)"
      ],
      "metadata": {
        "id": "WvFCql-xzeoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the limits for the plot based on the original data X\n",
        "limits_X1 = np.array([X[:,0].min(), X[:,0].max()])\n",
        "limits_X2 = np.array([X[:,1].min(), X[:,1].max()])\n",
        "\n",
        "# Add some padding to the plot limits for better visualization\n",
        "padding = 1.0\n",
        "x1_min, x1_max = limits_X1[0] - padding, limits_X1[1] + padding\n",
        "x2_min, x2_max = limits_X2[0] - padding, limits_X2[1] + padding\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# --- 1. Plot Decision Regions using a Meshgrid (recommended for multi-class) ---\n",
        "# Create a meshgrid to plot decision regions\n",
        "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
        "                       np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "# Flatten meshgrid points and add a bias term (1) for prediction\n",
        "mesh_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "mesh_points_with_bias = np.c_[np.ones((mesh_points.shape[0], 1)), mesh_points]\n",
        "\n",
        "# Predict probabilities for each point in the meshgrid using the learned parameters\n",
        "Y_pred_mesh_probs = softmax(parameter, mesh_points_with_bias)\n",
        "# Get the predicted class (index of max probability) for each point\n",
        "Z = np.argmax(Y_pred_mesh_probs, axis=1).reshape(xx1.shape)\n",
        "\n",
        "# Plot the decision regions using contourf\n",
        "plt.contourf(xx1, xx2, Z, alpha=0.3, cmap='viridis')\n",
        "\n",
        "# --- 2. Plot Original Data Points ---\n",
        "plt.scatter(X[:,0], X[:,1], c=Y.flatten(), cmap='viridis', alpha=0.8, edgecolors='k', label='Original Data')\n",
        "\n",
        "# --- 3. Plot Decision Boundaries (Lines) between each pair of classes ---\n",
        "# Helper function to find the line segment within plot bounds for a line Ax + By + C = 0\n",
        "def get_line_segment_in_bounds(A, B, C, x_lim, y_lim):\n",
        "    points = []\n",
        "\n",
        "    # Intersect with x = x_min\n",
        "    if B != 0:\n",
        "        y_intersect = (-C - A * x_lim[0]) / B\n",
        "        if y_lim[0] <= y_intersect <= y_lim[1]:\n",
        "            points.append((x_lim[0], y_intersect))\n",
        "    # Intersect with x = x_max\n",
        "    if B != 0:\n",
        "        y_intersect = (-C - A * x_lim[1]) / B\n",
        "        if y_lim[0] <= y_intersect <= y_lim[1]:\n",
        "            points.append((x_lim[1], y_intersect))\n",
        "\n",
        "    # Intersect with y = y_min\n",
        "    if A != 0:\n",
        "        x_intersect = (-C - B * y_lim[0]) / A\n",
        "        if x_lim[0] <= x_intersect <= x_lim[1]:\n",
        "            points.append((x_intersect, y_lim[0]))\n",
        "    # Intersect with y = y_max\n",
        "    if A != 0:\n",
        "        x_intersect = (-C - B * y_lim[1]) / A\n",
        "        if x_lim[0] <= x_intersect <= x_lim[1]:\n",
        "            points.append((x_intersect, y_lim[1]))\n",
        "\n",
        "    # Filter unique points (to handle corners or parallel lines more cleanly)\n",
        "    unique_points_tuples = list(set(points))\n",
        "    if len(unique_points_tuples) == 2:\n",
        "        return unique_points_tuples\n",
        "    return None # No valid segment found\n",
        "\n",
        "# Iterate through all unique pairs of classes to find their decision boundaries\n",
        "for i in range(n_classes):\n",
        "    for j in range(i + 1, n_classes): # Loop only for unique pairs (i, j) where i < j\n",
        "        # The decision boundary between class i and class j is given by:\n",
        "        # (theta_i - theta_j)^T * X = 0\n",
        "        # Let delta_theta = theta_i - theta_j\n",
        "        # delta_theta[0] + delta_theta[1]*x1 + delta_theta[2]*x2 = 0\n",
        "        delta_theta = parameter[:, i] - parameter[:, j] # delta_theta is a (features+1,) vector\n",
        "\n",
        "        # Coefficients for the line equation A*x1 + B*x2 + C = 0\n",
        "        C = delta_theta[0] # Intercept (bias)\n",
        "        A = delta_theta[1] # Coefficient for X1\n",
        "        B = delta_theta[2] # Coefficient for X2\n",
        "\n",
        "        # Get the line segment within the plot bounds\n",
        "        segment_points = get_line_segment_in_bounds(A, B, C, (x1_min, x1_max), (x2_min, x2_max))\n",
        "\n",
        "        if segment_points:\n",
        "            x_vals = [p[0] for p in segment_points]\n",
        "            y_vals = [p[1] for p in segment_points]\n",
        "            plt.plot(x_vals, y_vals, color='red', linestyle='--', linewidth=2,\n",
        "                     label=f'Boundary {i} vs {j}')\n",
        "\n",
        "plt.title('Softmax Regression Decision Regions and Boundaries')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.xlim(x1_min, x1_max)\n",
        "plt.ylim(x2_min, x2_max)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Oi89wYawzoNw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}