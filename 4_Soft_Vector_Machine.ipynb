{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMISQolpktZRxaSDge+QqAz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeonUI/ML_CS229/blob/main/4_Soft_Vector_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4lw_0vSzym5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Support Vector Machines\n",
        "\n",
        "Calculate **Hyperplane (Support Vector)** that divides two outputs\n",
        "\n",
        "- Problem: Binary classification $x \\mapsto \\{-1,1\\}$\n",
        "\n",
        "- Goal: Increase Functional margin of hyperplane (Geometric margin)\n",
        "\n",
        "- Hyperplane: $(w,b)$  ($w^Tx+b = 0$)\n",
        "\n",
        "- $h_{w, b}(x) = g(w^Tx+b)$\n",
        "\n",
        "- $g(z) = 1\\space \\text{or}\\space-1$\n",
        "\n",
        "\n",
        "\n",
        "**Functional margin**\n",
        "\n",
        "- Distance between hyperplane and example $(x^{(i)},y^{(i)})$\n",
        "\n",
        "$$\n",
        "\\hat{\\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)}+b) \\\\\n",
        "\\hat{\\gamma} = \\min \\hat{\\gamma}^{(i)}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Geometrical margin**\n",
        "\n",
        "- Normalized functional margin\n",
        "\n",
        "$$\n",
        "{\\gamma}^{(i)} = \\frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||} \\\\\n",
        "\\gamma = \\min \\gamma^{(i)}\n",
        "$$\n",
        "\n",
        "###How to study\n",
        "Set closest functional margin is equal to zero $y^{(i)}(w^Tx^{(i)}+b) = 1$ ...\n",
        "$$\n",
        "{\\gamma}^{(i)} = \\frac{1}{||w||}\n",
        "$$\n",
        "\n",
        "- Minimize $||w||$\n",
        "- subject to $y^{(i)}(w^Tx^{(i)}+b) \\ge 1$\n"
      ],
      "metadata": {
        "id": "LTbDTulxz4mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "np.random.seed(0)\n",
        "N_samples = 50\n",
        "n_features = 2\n",
        "n_classes = 2\n",
        "X, Y = make_blobs(n_samples=N_samples, n_features=n_features, centers=n_classes,\n",
        "                  cluster_std=1.5, random_state=42)\n",
        "\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# Y = iris.target\n",
        "\n",
        "Y = np.where(Y == 0, -1, 1)\n",
        "Y = Y.reshape(-1, 1)\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "X = std_scaler.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=Y.flatten(), label='Data', alpha=0.8)\n",
        "plt.title('Synthetic Data for Softmax Regression (3 Classes)')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.colorbar(label='Class Label')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# X[training_set, features], Y[training_set]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Ctum8wcZN-um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lagrange Multiplier\n",
        "1. Transform to Lagrangian\n",
        "    - Minimize $||w||$\n",
        "    - subject to $y^{(i)}(w^Tx^{(i)}+b) \\ge 1$\n",
        "\n",
        "$$\n",
        "ùìõ(w,b,\\alpha) = \\frac12 ||w||^2 - \\sum_{i=1}^m \\alpha_i\\left[y^{(i)}(w^Tx^{(i)}+b) -1 \\right]\n",
        "\\\\ \\text{s.t. } \\alpha_i \\ge 0\n",
        "$$\n",
        "\n",
        "2. Derivations equal zero\n",
        "$$\n",
        "\\nabla_w ùìõ(w,b,\\alpha) = w-\\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)} = 0 \\quad \\\\ \\therefore w = \\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)}\n",
        "\\\\\n",
        "\\frac{‚àÇ}{\\partial b} = \\sum_{i=1}^m \\alpha_i y^{(i)} = 0\n",
        "$$\n",
        "\n",
        "3. Substitute primary equation with above\n",
        "$$\n",
        "\\max\\sum_i\\alpha_i -\\frac12\\sum_i\\sum_j\\alpha_i\\alpha_jy^{(i)}y^{(j)} \\left< x^{(i)}, x^{(j)} \\right> \\quad \\\\ \\text{s.t. } \\alpha_i \\ge 0, \\quad\n",
        "\\sum_i \\alpha_i y^{(i)}=0\n",
        "$$\n",
        "\n",
        "4. Learn for alpha\n",
        "5. Get w with $w = \\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)}$\n",
        "\n",
        "### $L_1$ norm soft margin SVM - Regularization\n",
        "- cost xi, constant C\n",
        "\n",
        "$$\n",
        "\\min _{\\gamma, w, b}  \\frac{1}{2}\\|w\\|^2+C \\sum_{i=1}^m \\xi_i \\\\\n",
        "\\text { s.t. } y^{(i)}\\left(w^T x^{(i)}+b\\right) \\geq 1-\\xi_i, \\\\ i=1, \\ldots, m  \\quad \\xi_i \\geq 0, \\quad i=1, \\ldots, m\n",
        "$$\n",
        "\n",
        "- same process.. turns to below\n",
        "$$\n",
        "W(\\alpha) = \\max\\sum_i\\alpha_i -\\frac12\\sum_i\\sum_j\\alpha_i\\alpha_jy^{(i)}y^{(j)} \\left< x^{(i)}, x^{(j)} \\right> \\quad \\\\ \\text{s.t. } 0 \\le \\alpha_i \\le C, \\quad\n",
        "\\sum_i \\alpha_i y^{(i)}=0\n",
        "$$\n",
        "‚Üí Only difference is constraint of a_i **(why)**\n",
        "\n",
        "###SMO Algorithm - How to learn $\\alpha$\n",
        "\n",
        "- Sequntial minimal optimization algorithm\n",
        "\n",
        "1. Choose two alphas to optimize $\\alpha_1, \\alpha_2$\n",
        "    - Want to optimize Œ± one by one sequentially,\n",
        "   but, because of this equation, $\\sum_i \\alpha_i y^{(i)}=0$ equation, optimize two by one.\n",
        "\n",
        "2. Find maximum value of $W(\\alpha) = W\\left((\\zeta-\\alpha_2 y^{(2)}) y^{(1)}, \\alpha_2, ... \\alpha_m \\right)$\n",
        "    - it's just Quadratic function\n",
        "    - $\\alpha_1y^{(1)} + \\alpha_2 y^{(2)} = \\zeta$\n",
        "\n",
        "3. Clipping: Alpha should be inside of (0, C)\n",
        "\n",
        "4. Repeat until Convergence\n",
        "\n",
        "\n",
        "### Choosing two alphas\n",
        "\n",
        "1. Select alpha1 violates Karush-Kuhn-Tucker (KKT) conditions\n",
        "    - $0 \\ge \\alpha_i \\ge C$\n",
        "    - $\\alpha_i y_i = 1 (apply tol)\n",
        "\n",
        "2. Select alpha2 maximizes next eqn.\n",
        "    - |E_1 - E_2|\n",
        "    - $E_1 = pred(X^{(1)}) - y^{(1)}$\n",
        "\n",
        "### Updating two alphas (+ bias)\n",
        "1. Calculate kernel difference $\\eta$\n",
        "    - $\\eta = \\left<x_1, x_1 \\right> + \\left<x_2, x_2 \\right> -2\\left<x_1, x_2 \\right> $\n",
        "\n",
        "2. Calculate new alphas\n",
        "    - $\\alpha_{2\\text{new}} := \\alpha_2 + y_2\\frac{E_1 - E_2}{\\eta}$\n",
        "\n",
        "    - $\\alpha_1 := \\alpha_1 + y_1y_2d_2$\n",
        "\n",
        "    - $d_2 = \\alpha_2 - \\alpha_{2\\text{new}}$\n",
        "\n",
        "3. Clipping alphas\n",
        "    - clip alphas so that both alphas fit in [0,C]\n",
        "    - consider alpha2's value when clipping alpha1\n",
        "\n",
        "4. Calculate new bias\n",
        "    - $b_\\text{new} := b - E_1 - y_1d_1\\left<x_1, x_1 \\right> - y_2d_2\\left<x_1, x_2 \\right>$\n",
        "\n",
        "    - $b_\\text{new} := b - E_2 - y_1d_1\\left<x_1, x_2 \\right> - y_2d_2\\left<x_2, x_2 \\right>$\n",
        "\n",
        "    - Choose b in the range [0,C].\n",
        "     If both all in the range, take average\n",
        "\n",
        "### Kernel\n",
        "Can use feature space $ \\phi(x) = \\{x_1, x_2, x_1x_2, x_1^2 ... \\}$\n",
        "\n",
        "We replace all $ \\left<x, z \\right> = x^Tz $ with $K(x,z)$\n",
        "$$\n",
        "\\left<x, z \\right> = x^Tz \\text{(linear Kernel)} \\\\\n",
        "K(x,z) = \\phi(x)^T\\phi(z)\n",
        "$$"
      ],
      "metadata": {
        "id": "rDU5C5zFpyo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_kernel(x,z):\n",
        "    return np.dot(x, z.T)\n",
        "\n",
        "# return Prediction using dual form -> np.array[# samples]\n",
        "def predict(X_val, Alpha, Y, X_train, b, kernel) -> np.array:\n",
        "    # X_val can be a single sample (n_features,) or a batch of samples (n_samples, n_features)\n",
        "    if X_val.ndim == 1:\n",
        "        X_val_2d = X_val.reshape(1, -1)\n",
        "    else:\n",
        "        X_val_2d = X_val\n",
        "\n",
        "    m = X_train.shape[0]\n",
        "    preds = np.zeros((X_val_2d.shape[0], 1))\n",
        "\n",
        "    for i in range(X_val_2d.shape[0]):\n",
        "        prediction = 0\n",
        "        for j in range(m):\n",
        "            prediction += Alpha[j] * Y[j] * kernel(X_val_2d[i], X_train[j])\n",
        "        preds[i] = prediction + b\n",
        "    return preds\n",
        "\n",
        "# return two alphas\n",
        "def choose_alpha(Alpha, X_train, Y_train, b, C, tol, kernel) -> tuple[int,int]:\n",
        "    m = X_train.shape[0]\n",
        "\n",
        "    for i in range(m):\n",
        "        pred_xi = predict(X_train[i], Alpha, Y_train, X_train, b, kernel)\n",
        "        y_i = Y_train[i]\n",
        "        alpha_i = Alpha[i]\n",
        "\n",
        "        # KKT conditions for alpha_i\n",
        "        kkt_violation = False\n",
        "        if alpha_i <= 0:\n",
        "            kkt_violation = True\n",
        "        elif alpha_i >= C:\n",
        "            kkt_violation = True\n",
        "        elif abs(y_i * pred_xi - 1) > tol:\n",
        "            kkt_violation = True\n",
        "\n",
        "        if kkt_violation:\n",
        "            alpha1_idx = i\n",
        "            alpha2_idx = -1\n",
        "            error_i = pred_xi - y_i\n",
        "            max_error = tol\n",
        "            # Find second alpha - find max error diff j\n",
        "            for j in range(m):\n",
        "                if i == j: continue\n",
        "                pred_xj = predict(X_train[j], Alpha, Y_train, X_train, b, kernel)\n",
        "                error_j = pred_xj - Y_train[j]\n",
        "                error_ij = abs(error_i - error_j)\n",
        "\n",
        "                if error_ij > max_error:\n",
        "                    alpha2_idx = j\n",
        "                    max_error = error_ij\n",
        "\n",
        "            if alpha2_idx != -1:\n",
        "                return alpha1_idx, alpha2_idx\n",
        "\n",
        "    return -1, -1\n",
        "\n",
        "def update_alpha(X_train, Y_train, Alpha, b, alpha1_idx, alpha2_idx, C, kernel, tol_alpha=1e-5):\n",
        "    alpha1 = Alpha[alpha1_idx].copy()\n",
        "    alpha2 = Alpha[alpha2_idx].copy()\n",
        "    y1, y2 = Y_train[alpha1_idx], Y_train[alpha2_idx]\n",
        "    x1, x2 = X_train[alpha1_idx], X_train[alpha2_idx]\n",
        "\n",
        "    # Calculate E_i and E_j using the current model's prediction\n",
        "    E1 = predict(x1, Alpha, Y_train, X_train, b, kernel) - y1\n",
        "    E2 = predict(x2, Alpha, Y_train, X_train, b, kernel) - y2\n",
        "\n",
        "    # Calculate eta, kernel distance between i, j\n",
        "    K11 = kernel(x1, x1)\n",
        "    K22 = kernel(x2, x2)\n",
        "    K12 = kernel(x1, x2)\n",
        "    eta = K11 + K22 - 2 * K12\n",
        "    if eta <= 0:\n",
        "        return b\n",
        "\n",
        "    # Calculate L, H\n",
        "    #alpha1 - alpha2 = const\n",
        "    if y1 != y2:\n",
        "        L = max(0, alpha2 - alpha1)\n",
        "        H = min(C, C + alpha2 - alpha1)\n",
        "    #alpha1 + alpha2 = const\n",
        "    elif y1 == y2:\n",
        "        L = max(0, alpha1 + alpha2 - C)\n",
        "        H = min(C, alpha1 + alpha2)\n",
        "\n",
        "    # Calculate new alpha1, 2\n",
        "    alpha2_new_unclipped = alpha2 + y2 * (E1 - E2) / eta\n",
        "    alpha2_new = np.clip(alpha2_new_unclipped, L, H)\n",
        "    if abs(alpha2_new - alpha2) < tol_alpha:\n",
        "        return b\n",
        "\n",
        "    alpha1_new = alpha1 + y1 * y2 * (alpha2 - alpha2_new)\n",
        "\n",
        "    # Update\n",
        "    Alpha[alpha1_idx] = alpha1_new\n",
        "    Alpha[alpha2_idx] = alpha2_new\n",
        "\n",
        "    b1 = b - E1 - y1 * K11 * (alpha1_new - alpha1) - y2 * K12 * (alpha2_new - alpha2)\n",
        "    b2 = b - E2 - y1 * K12 * (alpha1_new - alpha1) - y2 * K22 * (alpha2_new - alpha2)\n",
        "\n",
        "    if 0 < alpha1_new < C:\n",
        "        b_new = b1\n",
        "    elif 0 < alpha2_new < C:\n",
        "        b_new = b2\n",
        "    else: # Both alpha_i_new and alpha_j_new are at their bounds (0 or C), take average\n",
        "        b_new = (b1 + b2) / 2.0\n",
        "    return b_new\n",
        "\n",
        "def get_w(Alpha, X_train, Y_train) -> np.array:\n",
        "    return np.sum(Alpha * Y_train * X_train, axis=0).reshape(-1, 1)\n",
        "\n",
        "def svm(X_train, Y_train, limit, C=1.0, tol=1e-3, kernel=linear_kernel):\n",
        "    m, n = X_train.shape\n",
        "    Alpha = np.zeros((m, 1))\n",
        "    b = 0\n",
        "\n",
        "    for i in range(limit):\n",
        "        alpha1_idx, alpha2_idx = choose_alpha(Alpha, X_train, Y_train, b, C, tol, kernel)\n",
        "\n",
        "        if (alpha1_idx == -1) and (alpha2_idx == -1):\n",
        "            break\n",
        "\n",
        "        b = update_alpha(X_train, Y_train, Alpha, b, alpha1_idx, alpha2_idx, C, kernel)\n",
        "\n",
        "    w = get_w(Alpha, X_train, Y_train)\n",
        "    return w, b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "w, b = svm(X_train, Y_train, 1000)\n",
        "print (w, b)"
      ],
      "metadata": {
        "id": "67nFvJ7Puta9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assume w, b, X_train, Y_train are available from the kernel state\n",
        "\n",
        "# Select the same two features used in the initial scatter plot (X[:, 0], X[:, 3])\n",
        "feature_idx1 = 0\n",
        "feature_idx2 = 1\n",
        "\n",
        "# Extract the scalar values for w components and b\n",
        "w0_val = w[feature_idx1].item()\n",
        "w3_val = w[feature_idx2].item()\n",
        "b_val = b.item()\n",
        "\n",
        "# Plot the training data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train[:, feature_idx1], X_train[:, feature_idx2], c=Y_train.flatten(), cmap='bwr', label='Training Data', alpha=0.8)\n",
        "\n",
        "# Define the range for the decision boundary line\n",
        "x_min, x_max = X_train[:, feature_idx1].min() - 0.5, X_train[:, feature_idx1].max() + 0.5\n",
        "xx = np.linspace(x_min, x_max, 100)\n",
        "\n",
        "# Calculate the decision boundary for the selected features\n",
        "# The equation for the decision boundary is w[feature_idx1]*x_feature1 + w[feature_idx2]*x_feature2 + b = 0\n",
        "# Solving for x_feature2: x_feature2 = (-w[feature_idx1]*x_feature1 - b) / w[feature_idx2]\n",
        "\n",
        "epsilon = 1e-6 # Small value to prevent division by zero\n",
        "if abs(w3_val) < epsilon: # If w3 is negligible, the decision boundary is primarily vertical in this 2D projection\n",
        "    if abs(w0_val) < epsilon: # Both w0 and w3 are negligible for the projection\n",
        "        plt.text((x_min + x_max)/2, X_train[:, feature_idx2].mean(), \"Cannot plot clear boundary (w0 and w3 near zero)\", horizontalalignment='center')\n",
        "    else:\n",
        "        # Plot a vertical line. Y-range is defined by the data's Y-axis range.\n",
        "        plt.axvline(x=(-b_val / w0_val), color='k', linestyle='--', label='Decision Boundary')\n",
        "        plt.ylim(X_train[:, feature_idx2].min() - 0.5, X_train[:, feature_idx2].max() + 0.5)\n",
        "else:\n",
        "    # Calculate the y-values for the decision boundary line\n",
        "    yy = (-w0_val * xx - b_val) / w3_val\n",
        "    plt.plot(xx, yy, 'k--', label='Decision Boundary')\n",
        "\n",
        "plt.title(f'SVM Decision Boundary (2D Projection of Features {feature_idx1+1} and {feature_idx2+1})')\n",
        "plt.xlabel(f'Feature {feature_idx1+1}')\n",
        "plt.ylabel(f'Feature {feature_idx2+1}')\n",
        "plt.colorbar(label='Class Label')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Re6eZXZgURDn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}